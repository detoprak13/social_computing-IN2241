{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c4b3c1",
   "metadata": {},
   "source": [
    "# Social Computing/Social Gaming - Summer 2023\n",
    "# Exercise Sheet 4 - Social Tie Strength\n",
    "\n",
    "In this exercise, you are going to predict Tie Strength in a social network using the method explained in the paper _E. Gilbert and K. Karahalios: Predicting Tie Strength With Social Media_ [1], of which a short introduction is provided to you in the exercise files. According to Mark Granovetter, the strength of a tie between two persons is a combination of the amount of time, the emotional intensity, the intimacy and the reciprocal services which characterize it. Using variables that describe these categories, we want to find out how much each one of these features contributes in order to predict the strength of ties not previously known.<br>\n",
    "An important prerequisite to this exercise is understanding the basic concept of linear regression models. As mentioned in the lecture, a recommended reading is chapter 3 of _C. Bishop: Pattern Recognition and Machine Learning_ [2], which you can find on [Moodle](https://www.moodle.tum.de/) [3].\n",
    "\n",
    "\n",
    "### Tie Strength Prediction\n",
    "\n",
    "In social network analysis, the Tie Strength between two people measures how strong their relationship is. The paper above describes the procedure of deriving available information (different variables) about a connection between two persons from an online social network and using it in order to discover how close they are. The ultimate goal is to build a model using the given information, finding out which variables account most for the Tie Strength and using that model later on to predict social Tie Strength when only the predictive (or explanatory) variables are available. Before being able to predict anything, we need to find out whether the given variables are suitable for prediction in the first place. This can be done via creating and evaluating a **multiple linear regression model**. 'Multiple' here refers to having more than one predictive variable in an regression model.<br>\n",
    "In the paper mentioned above, 67 variables where used in the linear model to predict the Tie Strength. In our simplified model, we are going to use only 10 predictive variables which are:\n",
    "* number of friends\n",
    "* number of friends's friends\n",
    "* days since last communication\n",
    "* shared appearances in photos\n",
    "* wall intimacy words\n",
    "* inbox intimacy words\n",
    "* days since first communication\n",
    "* number of mutual friends\n",
    "\n",
    "We are going to use a simplified form of the paper's linear model:\n",
    "$$y_i = \\alpha + \\beta X_i + \\epsilon_i$$\n",
    "\n",
    "where $y_i$ is the dependent variable (also referred to as target value, which is the Tie Strength in our case) of the $i$-th friend of a person. $X_i$ is the predictive vector, containing the (predictive) variables listed above. $\\alpha$ and $\\beta$ are the model's parameters, where $\\alpha$ is the intercept/bias, $\\beta$ the coefficient vector containing coefficients for each predictive variable, and $\\epsilon$ the prediction error. The regression problem boils down to calculating the model's parameters given a certain ground truth; meaning that for some connections, the Tie Strength has to be already known for building the model. That way, the unknown Tie Strengths can be predicted using the regression model by simply inserting the values into the vector. The coefficients for each predictive variable will show us the importance of the respective variable for the social Tie Strength.\n",
    "\n",
    "### Problem Overview\n",
    "\n",
    "The input to your Python program is a directed social network _SocialGraph.gml_. As the first step, you will visualize the graph with NetworkX to get an overview over the data.\n",
    "\n",
    "In practice, the ground truth (Tie Strength in our case) is usually retrieved by participant's answers to surveys on how strong their relationship is with another person - this is why the graph is directed: two people might have varying views. The ground truth is available in the file. About 70% of the edges have valid values for the `tieStrength` variable, which should be used for training. For about 30% of the edges, the variable is set to -1 (equivalent to unknown). These represent the prediction set for which the Tie Strength should be predicted using the linear regression model later. But first, that model needs to be computed and checked for its goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690037f4",
   "metadata": {},
   "source": [
    "## Task 4.1: Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7b2ae",
   "metadata": {},
   "source": [
    "### a) Imports and Visualization\n",
    "First, needed libraries and the graph's .gml file have to be imported. The social graph is visualized in order to get an idea what the network actually looks like.\n",
    "Inspect the plotted graph. **Describe** shortly, what the graph's visualization is telling you, and if there are any problems with this representation. **Any ideas** on how to improve the visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "import networkx as nx, numpy as np, pandas as pd, statsmodels.api as sm, matplotlib.pyplot as plt\n",
    "\n",
    "# read in the structure\n",
    "g = nx.read_gml('SocialGraph.gml', label='id')\n",
    "\n",
    "\n",
    "# formatting the graph and applying spring layout\n",
    "fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "pos=nx.spring_layout(g, k=0.4, iterations=5)\n",
    "\n",
    "visual_style = {\n",
    "    \"node_size\": 300,\n",
    "    \"node_color\": \"#4089EF\",\n",
    "    \"bbox\" : (700,700), \n",
    "    \"with_labels\" : False\n",
    "}\n",
    "\n",
    "nx.draw(g, pos, **visual_style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaefa1f",
   "metadata": {},
   "source": [
    "**TODO: Write your observations and ideas here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb41bb",
   "metadata": {},
   "source": [
    "### b) Complete and convert the data\n",
    "\n",
    "To further work with our data set, we will now convert it to a [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) [4] dataframe. \n",
    "Some of our predictive variables are not yet computed in the _gml_ file, therefore you have to **calculate the missing variables** from the graph's attributes. You can take a look at the _gml_ file as it is human-readable to see what variables are available for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the missing values for current edge e of graph g\n",
    "def calculate_missing_variables(g, e):\n",
    "    # The both nodes connected by edge e\n",
    "    first, second = e\n",
    "    # Edge data such as firstComm and tieStrength\n",
    "    edge_data = g.get_edge_data(first, second)\n",
    "    \n",
    "    # Source and target nodes for current edge\n",
    "    src = g.nodes[first]\n",
    "    tgt = g.nodes[second]\n",
    "        \n",
    "    # Already existing variables\n",
    "    days_last_comm = edge_data['lastComm']\n",
    "    photos_together = edge_data['photosTogether']\n",
    "    wall_intim_words = edge_data['wallIntimWords']\n",
    "    inbox_intim_words = edge_data['inboxIntimWords']\n",
    "    days_first_comm = edge_data['firstComm']\n",
    "    \n",
    "    # The Ground Truth\n",
    "    tie_strength = edge_data['tieStrength']\n",
    "\n",
    "    \n",
    "    # TODO: Compute the missing values\n",
    "    num_friends = len(list(g.neighbors(first)))  # number of friends of 'first'\n",
    "    friends_num_friends = sum([len(list(g.neighbors(n))) for n in g.neighbors(first)])  # sum of number of friends of each friend of 'first'\n",
    "    num_mutual_friends = len(list(nx.common_neighbors(g.to_undirected(), first, second)))  # number of mutual friends of 'first' and 'second'\n",
    "    \n",
    "    # Assuming age and education attributes exist in the node data. Replace 'age' and 'education' with correct attribute names.\n",
    "    age_dist = abs(src['age'] - tgt['age'])  # age difference between 'first' and 'second'\n",
    "    edu_diff = abs(src['numAcDegrees'] - tgt['numAcDegrees'])  # educational difference between 'first' and 'second'\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create row for dataframe\n",
    "    row = [num_friends, friends_num_friends, days_last_comm, photos_together, wall_intim_words, inbox_intim_words, days_first_comm, num_mutual_friends, age_dist, edu_diff]\n",
    "    row = [int(attr) for attr in row]\n",
    "    row.append(tie_strength) # Appended separately, needs to be float\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "#TODO: modify the code to correctly split the data\n",
    "# Training and prediction lists\n",
    "train_list = []\n",
    "pred_list = []\n",
    "cols = ['#Friends', 'Friends\\' #Friends', '#Days Since Last Comm', '#Photos', '#Wall Intimacy Words', '#Inbox Intimacy Words', '#Days Since First Comm','#Mutual Friends', 'Age Dist', 'Educational Diff', 'Tie Strength']\n",
    "\n",
    "# Calculate rows (one for each edge) and add them to tables\n",
    "\n",
    "for e in g.edges:\n",
    "    row = calculate_missing_variables(g, e)\n",
    "    \n",
    "    first, second = e\n",
    "    edge = g.get_edge_data(first, second)\n",
    "    \n",
    "    if edge['tieStrength'] != -1:\n",
    "        train_list.append(row)\n",
    "    else:\n",
    "        pred_list.append(row)\n",
    "        \n",
    "# Create training and prediction tables\n",
    "train_table = pd.DataFrame(train_list, columns=cols)\n",
    "pred_table = pd.DataFrame(pred_list, columns=cols)\n",
    "train_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bedbcd8",
   "metadata": {},
   "source": [
    "### c) The Variance Inflation Factor (VIF)\n",
    "Multiple linear regression can hold some pitfalls if you do not evaluate your data beforehands. Such a pitfall is containing multicollinearity in your predictive variables. \n",
    "\n",
    "Find out and **explain** in your own words what multicollinearity is, why it forms a danger to linear regression models and how the VIF is linked to that. \n",
    "**Create** a temporary dataframe containing only the predictive variables and **add a constant value** to the dataframe for the VIF to produce representative values. Then **compute the VIFs** for them. Statsmodels `variance_inflation_factor()` and `add_constant()` will help you with that. \n",
    "\n",
    "Additionally **explain**: What do the results tell you? Do we have to make any adaptions deriving from them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a8c780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   feature  VIF\n",
      "0                    const  0.0\n",
      "1                 #Friends  inf\n",
      "2        Friends' #Friends  inf\n",
      "3    #Days Since Last Comm  inf\n",
      "4                  #Photos  inf\n",
      "5     #Wall Intimacy Words  inf\n",
      "6    #Inbox Intimacy Words  inf\n",
      "7   #Days Since First Comm  inf\n",
      "8          #Mutual Friends  inf\n",
      "9                 Age Dist  inf\n",
      "10        Educational Diff  inf\n",
      "11  Predicted Tie Strength  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\90534\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1781: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "C:\\Users\\90534\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "\n",
    "# TODO: Creat a dataframe, add a constant & compute VIF\n",
    "X = sm.add_constant(train_table.drop(columns=['Tie Strength']))\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2956bf",
   "metadata": {},
   "source": [
    "The Variance Inflation Factor (VIF) results show a high degree of multicollinearity, especially for '#Friends', 'Friends' #Friends', '#Photos', and '#Wall Intimacy Words'. This indicates that these variables are highly correlated with other predictors in our model, which could distort our understanding of their individual impacts on the target variable. To address this, we could consider removing one or more of these variables, creating a combined variable, or applying regularization techniques to mitigate the influence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e981a2",
   "metadata": {},
   "source": [
    "### d) Normalisation-Transformation\n",
    "\n",
    "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalisation is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. Not every dataset does require normalization, however as our dataset has variables with different ranges we need to normalize it.\n",
    "\n",
    "With the help of sklearn's preprocessing functionality,  **apply the normalisation-transformation on each feature vector for the training table (but not the Tie Strength)**. You can find more information about the preprocessing functionality [here](https://scikit-learn.org/stable/modules/preprocessing.html). [5] Again, output the first ten entries of your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ebe8357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Friends</th>\n",
       "      <th>Friends' #Friends</th>\n",
       "      <th>#Days Since Last Comm</th>\n",
       "      <th>#Photos</th>\n",
       "      <th>#Wall Intimacy Words</th>\n",
       "      <th>#Inbox Intimacy Words</th>\n",
       "      <th>#Days Since First Comm</th>\n",
       "      <th>#Mutual Friends</th>\n",
       "      <th>Age Dist</th>\n",
       "      <th>Educational Diff</th>\n",
       "      <th>Tie Strength</th>\n",
       "      <th>Predicted Tie Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.114478</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.179844</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.435982</td>\n",
       "      <td>0.279593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.216611</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.345786</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520420</td>\n",
       "      <td>0.373845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.042649</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.225022</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520384</td>\n",
       "      <td>0.384506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.076319</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.188532</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552931</td>\n",
       "      <td>0.474292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.543860</td>\n",
       "      <td>0.281494</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530087</td>\n",
       "      <td>0.418111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.081930</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.181581</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550754</td>\n",
       "      <td>0.393196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.305275</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.357081</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.426431</td>\n",
       "      <td>0.267999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.216334</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480201</td>\n",
       "      <td>0.327883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.229366</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589384</td>\n",
       "      <td>0.550684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.294052</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428290</td>\n",
       "      <td>0.209994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Friends  Friends' #Friends  #Days Since Last Comm  #Photos  \\\n",
       "0  0.197368           0.172989               0.114478     0.00   \n",
       "1  0.197368           0.172989               0.216611     0.12   \n",
       "2  0.197368           0.172989               0.042649     0.04   \n",
       "3  0.197368           0.172989               0.076319     0.28   \n",
       "4  0.197368           0.172989               0.088664     0.16   \n",
       "5  0.197368           0.172989               0.081930     0.04   \n",
       "6  0.197368           0.172989               0.305275     0.24   \n",
       "7  0.197368           0.172989               0.049383     0.00   \n",
       "8  0.197368           0.172989               0.043771     0.36   \n",
       "9  0.197368           0.172989               0.294052     0.12   \n",
       "\n",
       "   #Wall Intimacy Words  #Inbox Intimacy Words  #Days Since First Comm  \\\n",
       "0              0.162791               0.385965                0.179844   \n",
       "1              0.162791               0.192982                0.345786   \n",
       "2              0.232558               0.157895                0.225022   \n",
       "3              0.465116               0.280702                0.188532   \n",
       "4              0.139535               0.543860                0.281494   \n",
       "5              0.162791               0.771930                0.181581   \n",
       "6              0.302326               0.052632                0.357081   \n",
       "7              0.093023               0.228070                0.216334   \n",
       "8              0.558140               0.228070                0.229366   \n",
       "9              0.255814               0.122807                0.245873   \n",
       "\n",
       "   #Mutual Friends  Age Dist  Educational Diff  Tie Strength  \\\n",
       "0         0.135593  0.710526          0.333333      0.435982   \n",
       "1         0.152542  0.684211          0.000000      0.520420   \n",
       "2         0.050847  0.605263          0.000000      0.520384   \n",
       "3         0.067797  0.368421          0.000000      0.552931   \n",
       "4         0.101695  0.552632          0.000000      0.530087   \n",
       "5         0.067797  0.710526          0.000000      0.550754   \n",
       "6         0.050847  0.368421          0.333333      0.426431   \n",
       "7         0.067797  0.578947          0.000000      0.480201   \n",
       "8         0.067797  0.210526          0.000000      0.589384   \n",
       "9         0.067797  0.078947          0.333333      0.428290   \n",
       "\n",
       "   Predicted Tie Strength  \n",
       "0                0.279593  \n",
       "1                0.373845  \n",
       "2                0.384506  \n",
       "3                0.474292  \n",
       "4                0.418111  \n",
       "5                0.393196  \n",
       "6                0.267999  \n",
       "7                0.327883  \n",
       "8                0.550684  \n",
       "9                0.209994  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Apply normalization transformation\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() \n",
    "\n",
    "# Specify the columns to be normalized\n",
    "features_to_scale = train_table.columns.difference(['Tie Strength'])\n",
    "\n",
    "train_table[features_to_scale] = scaler.fit_transform(train_table[features_to_scale])\n",
    "\n",
    "train_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47da69",
   "metadata": {},
   "source": [
    "## Task 4.2: The Regression Model\n",
    "\n",
    "### a) Building the model\n",
    "**1.**\n",
    "Finally, the regression can be applied on the dataframe. For this purpose, **split** the dataframe into `y`: the target variable and `X`: the predictive variables. As you have read above, our model contains a bias/intercept named $\\alpha$. This will be realized in the model by adding a constant (1.0), that gets multiplied with its own coefficient and therewith forms the intercept. It represents the target value when all explanatory variables are zero. Once again `add_constant(X)` will be of use.\n",
    "\n",
    "**Split** the dataframe, **add** the constant and then **apply** a multiple linear regression on the training table, the statsmodels functions `OLS()` and `fit()` will help you with that. Output the summary with `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3ab3347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           Tie Strength   R-squared:                       0.720\n",
      "Model:                            OLS   Adj. R-squared:                  0.720\n",
      "Method:                 Least Squares   F-statistic:                     1294.\n",
      "Date:                Sun, 25 Jun 2023   Prob (F-statistic):               0.00\n",
      "Time:                        21:30:50   Log-Likelihood:                 10043.\n",
      "No. Observations:                5038   AIC:                        -2.006e+04\n",
      "Df Residuals:                    5027   BIC:                        -1.999e+04\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      0.4475      0.002    251.856      0.000       0.444       0.451\n",
      "#Friends                  -0.0200      0.007     -2.740      0.006      -0.034      -0.006\n",
      "Friends' #Friends         -0.0027      0.007     -0.406      0.685      -0.016       0.011\n",
      "#Days Since Last Comm     -0.0077      0.004     -1.940      0.052      -0.015    8.02e-05\n",
      "#Photos                   -0.0021      0.008     -0.262      0.793      -0.018       0.014\n",
      "#Wall Intimacy Words       0.0367      0.007      5.237      0.000       0.023       0.050\n",
      "#Inbox Intimacy Words      0.0104      0.002      4.358      0.000       0.006       0.015\n",
      "#Days Since First Comm     0.0443      0.003     16.445      0.000       0.039       0.050\n",
      "#Mutual Friends            0.0256      0.004      6.991      0.000       0.018       0.033\n",
      "Age Dist                  -0.0008      0.002     -0.379      0.705      -0.005       0.003\n",
      "Educational Diff          -0.0251      0.002    -12.050      0.000      -0.029      -0.021\n",
      "Predicted Tie Strength     0.1271      0.002     57.850      0.000       0.123       0.131\n",
      "==============================================================================\n",
      "Omnibus:                       33.259   Durbin-Watson:                   2.013\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               47.008\n",
      "Skew:                          -0.060   Prob(JB):                     6.20e-11\n",
      "Kurtosis:                       3.458   Cond. No.                     7.33e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.94e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add constant & build the regression model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = train_table.drop('Tie Strength', axis=1)\n",
    "y = train_table['Tie Strength']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfd0d1",
   "metadata": {},
   "source": [
    "**2.**\n",
    "As you can see the model's summary provides us with a multitude of informations about its performance. Now we need to evaluate our model based on these values. Find out what the meaning of the following statistics are: `R-squared`, `Adj. R-squared`, `Prob (F-statistic)`, the predicitve variables' significances `P>|t|`. [This site](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/multiple-regression/interpret-the-results/key-results/) [6] does a good job explaining them intuitively.\n",
    "\n",
    "**Evaluate** our model's performance by giving a short comment on the obtained values for them. Don't write more than 5 sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e29ec",
   "metadata": {},
   "source": [
    "The model's R-squared value of 0.720 signifies that our predictors account for 72% of the 'Tie Strength' variance. A Prob (F-statistic) 0 indicates that the model as a whole is significant. However, the high p-values for 'Friends' #Friends', '#Photos', and 'Age Dist' suggest these predictors might not be significantly contributing to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25438e3d",
   "metadata": {},
   "source": [
    "**3.**\n",
    "Now additionally **compare** the obtained coefficients `coef` for our predictive variables to the findings of the paper referenced in [1]. Which kind of variables (Intimacy, Duration, Structural, Social distance) have the most influence on the Tie Strength according to our regression? You can also comment on specific predicitive variables' values. Keep in mind that the paper's coefficients are already standardized regarding the variabe's values, while ours do not yet compensate for them. Don't write more than 5 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd5c46",
   "metadata": {},
   "source": [
    "The model coefficients indicate that '#Days Since First Comm' (a Duration variable) has the highest positive impact on 'Tie Strength'. '#Friends' (a Structural variable) has the most substantial negative influence, and both 'Intimacy' variables ('#Wall Intimacy Words', '#Inbox Intimacy Words') show positive impacts. Compared to the referenced paper, our model also underlines the importance of Duration and Intimacy variables, with some differences in Structural variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0432b81",
   "metadata": {},
   "source": [
    "### b) OPTIONAL: Goodness of Fit\n",
    "After you have now analyzed some of the statistics of our model, there are some additional methods of analyzing the Goodness of Fit of our model. There are several methods to evaluate the Goodness of Fit of a regression. In this exercise, you will work with two of them: the Q-Q Plot and the Residual Plot.\n",
    "\n",
    "**1.: Q-Q Plot**\n",
    "\n",
    "Create a Q-Q Plot and evaluate what the result means for your fit. Plot the model's residuals on one axis and the normal distribution on the other axis, `scipy.stats` will provide it to you. What does the result tell you regarding your fit? Don't write more than 4 sentences.\n",
    "\n",
    "**Hint:** Statsmodles offers a function for Q-Q Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e08255ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# TODO: Create the QQ-Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f32b64",
   "metadata": {},
   "source": [
    "**TODO: Write your interpretation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fbedc",
   "metadata": {},
   "source": [
    "**2.: Residual Plot**\n",
    "\n",
    "Now evaluate your fit by plotting the residuals with matplotlib. The plot should show the standardized residuals for each entry. What does the result tell you regarding your fit? Don't write more than 4 sentences.\n",
    "\n",
    "**Hint:** The standardized residuals can be accessed via `model.resid_pearson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "799db6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the Residual-Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167dd78",
   "metadata": {},
   "source": [
    "**TODO: Write your interpretation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7493b40",
   "metadata": {},
   "source": [
    "## Task 4.3: Prediction of Tie Strengths\n",
    "As a last step, we want to compare be predicted tie strenghts with the true values using the before computed regression model. \n",
    "\n",
    "a) **Use the regression model to predict the Tie Strength values, for previously unseen data.** Statsmodels will be of help with that. **Remember** that we normalized the training data, so this needs to be done here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2692e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Friends</th>\n",
       "      <th>Friends' #Friends</th>\n",
       "      <th>#Days Since Last Comm</th>\n",
       "      <th>#Photos</th>\n",
       "      <th>#Wall Intimacy Words</th>\n",
       "      <th>#Inbox Intimacy Words</th>\n",
       "      <th>#Days Since First Comm</th>\n",
       "      <th>#Mutual Friends</th>\n",
       "      <th>Age Dist</th>\n",
       "      <th>Educational Diff</th>\n",
       "      <th>Tie Strength</th>\n",
       "      <th>Predicted Tie Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.801043</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709544</td>\n",
       "      <td>1.131325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.177329</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.669852</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716605</td>\n",
       "      <td>1.515165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233446</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.697654</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.710270</td>\n",
       "      <td>1.081002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>0.058361</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.329279</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724630</td>\n",
       "      <td>1.500360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.752656</td>\n",
       "      <td>0.179574</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.801043</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725713</td>\n",
       "      <td>1.436850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #Friends  Friends' #Friends  #Days Since Last Comm  #Photos  \\\n",
       "228   0.184211           0.063354               0.131313     0.28   \n",
       "659   0.289474           0.112291               0.177329     0.88   \n",
       "848   1.000000           1.000000               0.233446     0.36   \n",
       "1316  0.052632           0.009863               0.058361     0.88   \n",
       "1373  0.618421           0.752656               0.179574     0.28   \n",
       "\n",
       "      #Wall Intimacy Words  #Inbox Intimacy Words  #Days Since First Comm  \\\n",
       "228               0.348837               0.157895                0.801043   \n",
       "659               0.813953               0.087719                0.669852   \n",
       "848               0.488372               0.105263                0.697654   \n",
       "1316              0.976744               0.859649                0.329279   \n",
       "1373              0.441860               0.368421                0.801043   \n",
       "\n",
       "      #Mutual Friends  Age Dist  Educational Diff  Tie Strength  \\\n",
       "228          0.033898  0.052632          0.000000      0.709544   \n",
       "659          0.305085  0.026316          0.000000      0.716605   \n",
       "848          0.762712  0.078947          0.333333      0.710270   \n",
       "1316         0.050847  0.131579          0.000000      0.724630   \n",
       "1373         0.525424  0.157895          0.000000      0.725713   \n",
       "\n",
       "      Predicted Tie Strength  \n",
       "228                 1.131325  \n",
       "659                 1.515165  \n",
       "848                 1.081002  \n",
       "1316                1.500360  \n",
       "1373                1.436850  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform log transformation, add constant & predict the Tie Strengths\n",
    "\n",
    "# An example for queries:\n",
    "# pred_table[pred_table['Tie Strength'] > 0.7].head(5)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(train_table.drop('Tie Strength', axis=1))\n",
    "\n",
    "# Add the constant to the data\n",
    "normalized_data = sm.add_constant(normalized_data)\n",
    "\n",
    "# Use the model to predict the 'Tie Strength'\n",
    "predictions = model.predict(normalized_data)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "train_table['Predicted Tie Strength'] = predictions\n",
    "\n",
    "# Show some of the data\n",
    "train_table[train_table['Tie Strength'] > 0.7].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3be57",
   "metadata": {},
   "source": [
    "**Are the predictions in line with the observations above? Pick a few entries to back up your observations.** If you would like to talk about other than the first ten entries, you can query a pandas dataframe similar to SQL. More information on how to do this is available in the [pandas documenation](https://pandas.pydata.org/pandas-docs/version/0.19.2/comparison_with_sql.html) [4].\n",
    "\n",
    "As you might discover, there are some Tie Strength values slightly below zero. Can you **explain** that behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3df57",
   "metadata": {},
   "source": [
    "The predicted tie strengths seem higher than the actual values, suggesting our model overestimates the relationship strength. For instance, the first entry has an actual tie strength of 0.709544 but a predicted strength of 1.118510. Negative tie strengths, which fall outside the valid 0 to 1 range, could be due to model extrapolation beyond its training data or numerical precision issues during calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028228b0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] E. Gilbert and K. Karahalios: _Predicting Tie Strength With Social Media_. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2009.\n",
    "<br>[2] C. Bishop: _Pattern Recognition and Machine Learning_. 2006.\n",
    "<br>[3] https://www.moodle.tum.de/\n",
    "<br>[4] https://pandas.pydata.org/docs/user_guide/index.html\n",
    "<br>[5] https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "<br>[6] https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/multiple-regression/interpret-the-results/key-results/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
